# -*- coding: utf-8 -*-
"""by int

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CH6eleBYLDUZ2KXdBn7l3FKdtVmWIJig
"""

import pandas as pd


df = pd.read_csv("hotel_bookings.csv")  # Use the uploaded file name
df.drop_duplicates(inplace=True)

# ðŸ“Œ Step 3: Check for Missing Values
missing_values = df.isnull().sum()
print("Missing Values:\n", missing_values)
# ðŸ“Œ Step 4: Handle Missing Values
df['children'].fillna(0, inplace=True)  # Replace NaN with 0
df['country'].fillna('Unknown', inplace=True)  # Replace NaN with 'Unknown'
df['agent'].fillna(0, inplace=True)  # Replace NaN with 0 (Unknown Agent)
df['company'].fillna(0, inplace=True)  # Replace NaN with 0 (Unknown Company)

# ðŸ“Œ Step 5: Convert Data Types
df['reservation_status_date'] = pd.to_datetime(df['reservation_status_date'])

# ðŸ“Œ Step 6: Create New Features
df['revenue'] = (df['stays_in_weekend_nights'] + df['stays_in_week_nights']) * df['adr']

# Convert "arrival_date_year" and "arrival_date_month" to a single datetime column
df['arrival_date'] = pd.to_datetime(df['arrival_date_year'].astype(str) + '-' + df['arrival_date_month'] + '-1')

# ðŸ“Œ Step 7: Verify Data Cleaning
x = df.isnull().sum()
print("missing Values:\n", x)
print("\nUpdated Data Types:\n", df.dtypes)

df.head()

df.describe()

import matplotlib.pyplot as plt

# Revenue = stays in week & weekend nights * avg daily rate (adr)
df['revenue'] = (df['stays_in_weekend_nights'] + df['stays_in_week_nights']) * df['adr']

# Group by month-year
df['arrival_date'] = pd.to_datetime(df['arrival_date_year'].astype(str) + '-' + df['arrival_date_month'] + '-1')
monthly_revenue = df.groupby(df['arrival_date'])['revenue'].sum()

# Plot revenue trend
plt.figure(figsize=(12, 5))
plt.plot(monthly_revenue.index, monthly_revenue.values, marker='o')
plt.title('Revenue Trends Over Time')
plt.xlabel('Month')
plt.ylabel('Revenue')
plt.grid(True)
plt.show()

cancellation_rate = (df['is_canceled'].sum() / len(df)) * 100
print(f"Cancellation Rate: {cancellation_rate:.2f}%")

import seaborn as sns

# Country-wise booking count
country_counts = df['country'].value_counts().head(10)

# Plot
plt.figure(figsize=(10, 5))
sns.barplot(x=country_counts.index, y=country_counts.values)
plt.title('Top 10 Countries by Number of Bookings')
plt.xlabel('Country')
plt.ylabel('Number of Bookings')
plt.xticks(rotation=45)
plt.show()

sns.histplot(df['lead_time'], bins=50, kde=True)
plt.title('Booking Lead Time Distribution')
plt.xlabel('Days before Arrival')
plt.ylabel('Number of Bookings')
plt.show()

!pip install faiss-cpu

import pandas as pd
import sqlite3


# Convert date column to datetime
df['reservation_status_date'] = pd.to_datetime(df['reservation_status_date'])

# Compute revenue per booking
df['revenue'] = df['adr'] * (df['stays_in_weekend_nights'] + df['stays_in_week_nights'])

# Create SQLite database
conn = sqlite3.connect("hotel_data.db")
cursor = conn.cursor()

# Store Precomputed Insights
cursor.execute("""
    CREATE TABLE IF NOT EXISTS insights (
        key TEXT PRIMARY KEY,
        value TEXT
    )
""")

# Insert precomputed analytics into the database
precomputed_insights = {
    "total_revenue": df['revenue'].sum(),
    "highest_cancellations": df[df['is_canceled'] == 1]['country'].value_counts().idxmax(),
    "most_bookings": df['country'].value_counts().idxmax()
}

for key, value in precomputed_insights.items():
    cursor.execute("INSERT OR REPLACE INTO insights (key, value) VALUES (?, ?)", (key, str(value)))

conn.commit()
conn.close()

print("âœ… Insights Stored Successfully!")

import faiss
from sentence_transformers import SentenceTransformer

# Load pre-trained sentence embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Convert bookings to text format for retrieval
df['text_data'] = df.apply(lambda row:
    f"Booking from {row['country']} on {row['reservation_status_date']} "
    f"with ADR {row['adr']}. Lead time: {row['lead_time']} days. "
    f"Canceled: {'Yes' if row['is_canceled'] else 'No'}.", axis=1)

# Generate vector embeddings
embeddings = model.encode(df['text_data'].tolist(), convert_to_numpy=True)

# Initialize FAISS index
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

print("âœ… FAISS Vector Store Created!")

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load LLM (Mistral-7B in this case)
model_name = "tiiuae/falcon-7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
llm = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")

print("LLM loaded successfully!")

def query_database(query):
    """Fetch precomputed insights from SQLite"""
    conn = sqlite3.connect("hotel_data.db")
    cursor = conn.cursor()

    query = query.lower().replace(" ", "_")  # Normalize query key
    cursor.execute("SELECT value FROM insights WHERE key=?", (query,))
    result = cursor.fetchone()

    conn.close()
    return result[0] if result else None

def search_faiss(query, top_k=5):
    """Retrieve non-mathematical results from FAISS"""
    query_embedding = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_embedding, top_k)

    results = [df.iloc[i]['text_data'] for i in indices[0]]
    return results

def search_query(query, k=5):
    query_embedding = model.encode([query])
    distances, indices = index.search(np.array(query_embedding), k)

    results = df.iloc[indices[0]]
    return results[['hotel', 'arrival_date_month', 'arrival_date_year', 'country', 'revenue']]

query = "highest revenue in july 2016"
print(search_query(query))

import time
start = time.time()
search_query("Show me total revenue for July 2017")
print("Response time:", time.time() - start, "seconds")

# âœ… Initialize FastAPI
app = FastAPI()

class QueryRequest(BaseModel):
    question: str

def search_faiss(query, top_k=5):
    """ Retrieve relevant bookings using FAISS """
    query_embedding = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_embedding, top_k)
    results = [df.iloc[i]['text_data'] for i in indices[0]]
    return results

@app.post("/ask")
def ask_question(request: QueryRequest):
    query = request.question.lower()

    # âœ… Check for precomputed insights (numeric queries)
    if "total revenue" in query:
        return {"answer": f"The total revenue is ${precomputed_insights['total_revenue']:,.2f}."}

    elif "average price" in query:
        return {"answer": f"The average price of a hotel booking is ${precomputed_insights['average_price']:.2f}."}

    elif "highest cancellations" in query:
        return {"answer": f"The highest booking cancellations were from {precomputed_insights['highest_cancellations']}."}

    elif "most bookings" in query:
        return {"answer": f"The country with the most bookings is {precomputed_insights['most_bookings']}."}

    # âœ… If it's a general text-based query, use FAISS
    else:
        faiss_results = search_faiss(query)
        return {"answer": faiss_results}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)